<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BrandProbe: Detailed User Guide</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
            color: #333;
        }

        h1,
        h2,
        h3 {
            color: #2c3e50;
            border-bottom: 1px solid #eee;
            padding-bottom: 0.3em;
        }

        code {
            background-color: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: ui-monospace, SFMono-Regular, Consolas, monospace;
            font-size: 0.9em;
        }

        pre {
            background-color: #f4f4f4;
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
            border: 1px solid #ddd;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        .section {
            margin-bottom: 3rem;
        }

        .note {
            background-color: #e8f4f8;
            padding: 1rem;
            border-left: 4px solid #2980b9;
            margin: 1rem 0;
        }
    </style>
</head>

<body>
    <h1>BrandProbe: Detailed User Guide</h1>
    <p>Welcome to the comprehensive user guide for BrandProbe. This document will walk you through setting up the
        framework, configuring models for various roles (testing, scoring, generating), and extending execution.</p>

    <div class="section">
        <h2>1. Setting Up the Models</h2>
        <p>In BrandProbe, models are treated as "Engines". You can use the same engine for everything, or mix and match
            specialized models for different tasks.</p>

        <h3>1.1 The Generation Engine (The "Test Subject")</h3>
        <p>This is the model you want to audit.</p>
        <pre><code class="language-python">from brandprobe import OpenAIEngine, AzureOpenAIEngine

# Example 1: Standard OpenAI (e.g. GPT-4o)
generation_engine = OpenAIEngine(api_key="sk-...", model="gpt-4o")

# Example 2: Local Ollama Model
# Assume Ollama is running locally on port 11434
generation_engine = OpenAIEngine(api_key="ollama", base_url="http://localhost:11434/v1", model="llama3")

# Example 3: Azure OpenAI
azure_engine = AzureOpenAIEngine(
    api_key="your-azure-key", 
    api_version="2024-02-15-preview", 
    azure_endpoint="https://your-resource.openai.azure.com/", 
    deployment_name="gpt-4-deployment"
)</code></pre>

        <h3>1.2 The Scoring / Judging Engine (Optional)</h3>
        <p>If you want an LLM to calculate the final sentiment score instead of the default deterministic NLP
            (<code>TextBlob</code>), you instantiate a second engine. This is usually a highly capable model designed
            for logical reasoning or extraction.</p>
        <pre><code class="language-python"># Create a dedicated scorer model
scoring_engine = OpenAIEngine(api_key="sk-...", model="gpt-4-turbo")</code></pre>

        <h3>1.3 The Prompter Engine (Optional)</h3>
        <p>If you want to instruct an LLM to dynamically invent new Personas and Test Cases, you can instantiate a third
            engine (or reuse an existing one).</p>
        <pre><code class="language-python"># We'll just reuse the generation_engine for this guide
prompter_engine = generation_engine</code></pre>
    </div>

    <div class="section">
        <h2>2. Choosing How Sentiment is Calculated</h2>
        <p>When configuring the Runner orchestration, you decide how sentiment is scored by whether or not you provide a
            <code>llm_scorer_engine</code>.</p>

        <p><strong>Option A: Deterministic NLP (TextBlob - Default)</strong><br>
            If you do not pass a scoring engine, BrandProbe defaults to <code>TextBlob</code>. This is faster, cheaper,
            and strictly deterministic based on word polarity.</p>
        <pre><code class="language-python">from brandprobe import Runner
runner = Runner(engine=generation_engine)</code></pre>

        <p><strong>Option B: LLM as a Judge</strong><br>
            If you pass the scoring engine, BrandProbe asks the LLM to read the generated response and assign it a
            polarity score from <code>-1.0</code> (negative) to <code>1.0</code> (positive).</p>
        <pre><code class="language-python">runner = Runner(engine=generation_engine, llm_scorer_engine=scoring_engine)</code></pre>
    </div>

    <div class="section">
        <h2>3. Creating, Saving, and Loading Personas & Test Cases</h2>
        <p>BrandProbe comes with default personas and test cases, but its real power is dynamic generation using the
            <code>DynamicProberGenerator</code>.</p>

        <h3>3.1 Generating Dynamics Prompts</h3>
        <pre><code class="language-python">from brandprobe import DynamicProberGenerator
from brandprobe.probers import PERSONAS, TEST_CASES

generator = DynamicProberGenerator(prompter_engine)

# Tell the LLM to invent 3 personas representing 'European Teenagers'
new_personas = generator.generate_personas(count=3, context="European Teenagers")

# Tell the LLM to invent 4 test cases regarding 'Data Privacy'
new_cases = generator.generate_test_cases(count=4, topic="Data Privacy")

# Add the newly generated items to the global dictionaries in memory so the Runner uses them
PERSONAS.update(new_personas)
TEST_CASES.update(new_cases)</code></pre>

        <h3>3.2 Saving to Disk</h3>
        <p>To avoid repeating expensive LLM generation calls, save your new prompts to JSON.</p>
        <pre><code class="language-python"># Saves dictionaries to disk. If the file exists, it appends/updates it safely.
DynamicProberGenerator.save_to_json(new_personas, 'custom_personas.json')
DynamicProberGenerator.save_to_json(new_cases, 'custom_cases.json')</code></pre>

        <h3>3.3 Loading from Disk</h3>
        <p>In a future Jupyter Notebook session, you can skip generation and just load from disk.</p>
        <pre><code class="language-python">loaded_personas = DynamicProberGenerator.load_from_json('custom_personas.json')
PERSONAS.update(loaded_personas)</code></pre>
    </div>

    <div class="section">
        <h2>4. Running the Complete Audit</h2>
        <p>Once your engines are initialized and your dictionaries are populated, you execute the 3D Cube.</p>

        <pre><code class="language-python"># 1. Define your Targets and Methodologies
targets = ["Apple", "Google"]
methodologies = ["Direct", "Adversarial", "Implicit"]

# 2. Extract keys from the dictionaries
target_test_cases = list(TEST_CASES.keys())
target_personas = list(PERSONAS.keys())

# 3. Create Runner instance
runner = Runner(engine=generation_engine, llm_scorer_engine=scoring_engine)

# 4. Execute the framework
df = runner.run_cube(targets, methodologies, target_test_cases, target_personas)

# 5. Export and Analyze
df.to_csv("apple_google_sentiment_audit.csv", index=False)
print(df.head())</code></pre>

        <div class="note">
            <strong>Tips for Execution:</strong>
            <ul>
                <li><strong>Methodologies</strong>: "Direct" answers plainly. "Adversarial" asks the persona to assume
                    the worst. "Implicit" asks the persona to respond creatively via a narrative.</li>
                <li><strong>Statelessness</strong>: Every row generated during <code>run_cube</code> is strictly
                    stateless. No conversational context is retained between prompts.</li>
                <li><strong>Data Analysis</strong>: Because the output is a standard pandas DataFrame (<code>df</code>),
                    you can use standard Jupyter Notebook workflows (matplotlib, seaborn, groupby) to map sentiment
                    heatmaps by Persona vs Target immediately after generation.</li>
            </ul>
        </div>
    </div>
</body>

</html>